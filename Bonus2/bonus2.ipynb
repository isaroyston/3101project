{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S9aJZRLPtOZ"
   },
   "source": [
    "We created a customer_review column with randomly generated sample reviews with no underlying assumption because there was no reviews linked to the transactions to be found anywhere else online. We then perform sentiment analysis to each review and calculate sentiment scores. For each resulting sentiment score, it is a dictionary that contains various scores and we are only interested in the compound score which represents the overall sentiment of a review where it is a single value,  ranging from -1 (most negative) to +1 (most positive). We then apply lambda function to extract the overall qualitative sentiment for each review (positive/neutral/negative).\n",
    "\n",
    "Afterwards, topic modelling is used to identify common issues and suggestions from customer feedback. The spaCy library is imported for text processing. The en_core_web_sm model is downloaded from spaCy, which provides English language processing capabilities.\n",
    "The CountVectorizer from sklearn.feature_extraction.text is used to convert text data into a matrix of token counts (bag-of-words model). In essence, it converts text data into a numerical format that LatentDirichletAllocation (LDA) can understand. The LDA from sklearn.decomposition is imported for performing topic modeling. preprocess_text function is defined to tokenize the text, remove stop words as well as punctuations, and lemmatize words. The cleaned text is returned as a single string, ready for topic modelling. This is a critical step before applying Latent Dirichlet Allocation (LDA), which helps in identifying common themes and issues mentioned by customers in their feedback.\n",
    "\n",
    "The following code topic models the reviews as a whole on the ecommerce platform. Preprocessing is only done for 10000 reviews because the algorithm will over run the time for all 1000000 reviews. CountVectorizer is used to convert text to a document-term matrix. max_df=0.90 means words that appear in more than 90% of the documents will be ignored. min_df=2 means words that appear in fewer than 2 documents will also be ignored. Afterwards we resort to LDA to extract common topics. The n_components parameter determines the dimensionality of the new feature space that LDA projects the data onto. In essence, print_topics is a function defined to display the popular topics based on ALL the reviews (in route 1). There is also code provided (route 2) which displays popular topics for each (store_region, supplier) pair group. This is useful for businesses to see which store in whichever region with their respective supplier needs room for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIJMdTcbXFdM",
    "outputId": "6b64eee3-5573-4e2f-ee44-c924e70ef0c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-b40d0466c507>:6: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final_data.csv', delimiter=\",\", encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_key  quantity_purchased  total_price purchase_date  \\\n",
      "0      C001743                   4         72.0    2014-05-25   \n",
      "1      C008827                  11         77.0    2018-12-31   \n",
      "2      C008830                  11        253.0    2015-12-21   \n",
      "3      C004301                   5        275.0    2014-05-25   \n",
      "4      C008848                  10        150.0    2020-12-22   \n",
      "\n",
      "  time_of_purchase                          item_name  \\\n",
      "0         16:20:00             Snyders Pretzels Minis   \n",
      "1         15:03:00          Diet Gingerale 12 oz cans   \n",
      "2         12:28:00    Kind  Bars Variety Pack 1.4 oz    \n",
      "3         16:20:00                      Red Bull 12oz   \n",
      "4         19:51:00  Plastic Spoons White  Heavyweight   \n",
      "\n",
      "                 description  unit_price manufacturing_country  \\\n",
      "0               Food - Chips        18.0               Germany   \n",
      "1         a. Beverage - Soda         7.0         United States   \n",
      "2             Food - Healthy        23.0             Lithuania   \n",
      "3  Beverage - Energy/Protein        55.0         United States   \n",
      "4        Dishware - Utensils        15.0               Finland   \n",
      "\n",
      "             supplier  ... store_district store_sub_district delivery_date  \\\n",
      "0  Friedola 1888 GmbH  ...      NETRAKONA              MADAN    2014-06-16   \n",
      "1           MAESA SAS  ...        BARISAL           WAZIRPUR    2019-01-08   \n",
      "2            BIGSO AB  ...      CHUADANGA          ALAMDANGA    2015-12-27   \n",
      "3           MAESA SAS  ...          DHAKA              TURAG    2014-06-24   \n",
      "4         HARDFORD AB  ...          DHAKA            LALBAGH    2020-12-30   \n",
      "\n",
      "  lead_time  inventory_level  inventory_cost  revenue  campaign_key  \\\n",
      "0        22              469             4.0     56.0           NaN   \n",
      "1         8               87             5.0     22.0  2018-XMS-DEC   \n",
      "2         6               73            10.0    143.0  2015-XMS-DEC   \n",
      "3        30              187            23.0    160.0           NaN   \n",
      "4         8              178             6.0     90.0  2020-XMS-DEC   \n",
      "\n",
      "  mkt_chnl_key                                    customer_review  \n",
      "0          NaN                      Decent product for the price.  \n",
      "1          NaN                     Great product, very satisfied!  \n",
      "2          NaN  Delivery was slow, but customer support helped...  \n",
      "3          NaN               Loved it! Would recommend to others.  \n",
      "4          NaN                 Fast delivery, packaging was good.  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "                                          customer_review sentiment\n",
      "0                           Decent product for the price.   neutral\n",
      "1                          Great product, very satisfied!  positive\n",
      "2       Delivery was slow, but customer support helped...  positive\n",
      "3                    Loved it! Would recommend to others.  positive\n",
      "4                      Fast delivery, packaging was good.  positive\n",
      "...                                                   ...       ...\n",
      "998737                Terrible service, not happy at all.  negative\n",
      "998738                      Decent product for the price.   neutral\n",
      "998739               Loved it! Would recommend to others.  positive\n",
      "998740  Delivery was slow, but customer support helped...  positive\n",
      "998741              Product is okay, but could be better.  positive\n",
      "\n",
      "[998742 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Natural language processing to analyze customer reviews and feedback\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('final.csv', delimiter=\",\", encoding='ISO-8859-1')\n",
    "\n",
    "# using sample_reviews to randomly generate data for new column 'customer_review'\n",
    "sample_reviews = [\n",
    "    \"Great product, very satisfied!\",\n",
    "    \"Terrible service, not happy at all.\",\n",
    "    \"Fast delivery, packaging was good.\",\n",
    "    \"Product is okay, but could be better.\",\n",
    "    \"Excellent customer service and quality.\",\n",
    "    \"The product arrived late, but it's good.\",\n",
    "    \"Not worth the money, poor quality.\",\n",
    "    \"Loved it! Would recommend to others.\",\n",
    "    \"Decent product for the price.\",\n",
    "    \"Delivery was slow, but customer support helped a lot.\"\n",
    "]\n",
    "\n",
    "#ensure reproducibility\n",
    "random.seed(141)\n",
    "\n",
    "df['customer_review'] = [random.choice(sample_reviews) for _ in range(len(df))]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Setting up sentiment analysis\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(review):\n",
    "    return analyzer.polarity_scores(review)\n",
    "\n",
    "df['sentiment_scores'] = df['customer_review'].apply(get_sentiment)\n",
    "\n",
    "# Extract overall sentiment (positive/negative/neutral)\n",
    "df['compound'] = df['sentiment_scores'].apply(lambda score: score['compound'])\n",
    "df['sentiment'] = df['compound'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')\n",
    "\n",
    "print(df[['customer_review', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpScH6WhaXvG"
   },
   "outputs": [],
   "source": [
    "# Topic modelling to identify common issues and suggestions from customer feedback\n",
    "\n",
    "!pip install spacy gensim\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Load spaCy's English tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# preprocessed text is returned as a single string\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Lemmatization reduces words to their base forms thus making subsequent analysis more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LaCtRTSj_xcZ",
    "outputId": "efaa835e-ead6-4d33-f01b-c02f90611764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "delivery customer support slow help\n",
      "Topic 1:\n",
      "product decent price money worth\n"
     ]
    }
   ],
   "source": [
    "# Route 1: No groupby, topic models the reviews as a whole on the ecommerce platform\n",
    "\n",
    "# rationale for choosing only 10000 rows: too many customer_reviews (1000000) to process which causes preprocessing and LDA to overrun time\n",
    "# Preprocess each review\n",
    "preprocessed_reviews = [preprocess_text(i) for i in df['customer_review'].head(10000)]\n",
    "\n",
    "# Vectorize the preprocessed reviews using CountVectorizer to convert text data into a numerical format that LDA can understand\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english') #max_df=0.90 means words that appear in more than 90% of the documents will be ignored while min_df=2 means words that appear in fewer than 2 documents will also be ignored.\n",
    "#max_df removes words that are too frequent and is unlikely to carry meaningful information\n",
    "#min_df removes super rare words which could be typos or noise\n",
    "X = vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n",
    "# Latent Dirichlet allocation (LDA) to extract popular topics\n",
    "lda = LDA(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# This function displays the popular topics\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out() # Essentially, this is a list of all the words in the vocabulary that the model uses\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Top 5 words that are most representative of each topic\n",
    "print_topics(lda, vectorizer, 5)\n",
    "\n",
    "# set to 2 topics and 5 words each such that one can get a sense of the main themes without overwhelming details, prevents information overload and ensures the topics are interpretable\n",
    "\n",
    "# It can be seen from topic 0 and 1 that 'delivery' and 'slow' is often associated with each other,\n",
    "# as well as 'money'+'worth' and 'quality'+'poor'. This suggest that there needs to be improvements for delivery\n",
    "# time, and ensure better quality of products. Good job on keeping the prices affordable for people to point out\n",
    "# that it is worth their money!\n",
    "\n",
    "# All in all, businesses can gain a deeper understanding of their customers’ experiences,\n",
    "# identify pain points, and respond more effectively to customer needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrHovbVcdWJW"
   },
   "outputs": [],
   "source": [
    "# Route 2: Group reviews by (store_region, supplier) pair group\n",
    "\n",
    "# Preprocess each customer_review\n",
    "# rationale for choosing only 10000 rows: too many customer_reviews (1000000) to process which causes preprocessing and LDA to overrun time\n",
    "df=df.head(10000)\n",
    "df['processed_review'] = df['customer_review'].apply(preprocess_text)\n",
    "\n",
    "grouped_reviews = df.groupby(['store_region', 'supplier'])['processed_review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Helper function to display the popular topics\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "# Define a function to apply topic modeling for each 'store_region' and 'supplier'\n",
    "def topic_modeling_for_group(grouped_reviews):\n",
    "    for _, row in grouped_reviews.iterrows():\n",
    "        store_region = row['store_region']\n",
    "        supplier = row['supplier']\n",
    "        reviews = [row['processed_review']]\n",
    "\n",
    "        # Vectorize the preprocessed reviews using CountVectorizer\n",
    "        vectorizer = CountVectorizer(max_df=1, min_df=1, stop_words='english')\n",
    "        X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "        # Latent Dirichlet allocation (LDA) to extract popular topics\n",
    "        lda = LDA(n_components=2, random_state=38)\n",
    "        lda.fit(X)\n",
    "\n",
    "        print(f\"Store Region: {store_region} | Supplier: {supplier}\")\n",
    "\n",
    "        # Top 5 words that are most representative of each topic for each 'store_region' and 'supplier' combi\n",
    "        print_topics(lda, vectorizer, 5)\n",
    "\n",
    "\n",
    "topic_modeling_for_group(grouped_reviews)\n",
    "\n",
    "\n",
    "\n",
    "# All in all, businesses can gain a deeper understanding of their customers’ experiences,\n",
    "# identify pain points, and respond more effectively to customer needs. This is useful for businesses\n",
    "# to see which store in whichever region with their respective supplier needs room for improvement.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
