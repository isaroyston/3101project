{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S9aJZRLPtOZ"
   },
   "source": [
    "created a customer_review column with randomly generated sample reviews with no underlying assumption because there was no reviews linked to the transactions to be found anywhere else online. We then perform sentiment analysis to each review and calculate sentiment scores. For each resulting sentiment score, it is a dictionary that contains various scores and we are only interested in the compound score which represents the overall sentiment of a review where it is a single value,  ranging from -1 (most negative) to +1 (most positive). We then apply lambda function to extract the overall qualitative sentiment for each review (positive/neutral/negative).\n",
    "\n",
    "Afterwards, topic modelling is used to identify common issues and suggestions from customer feedback. The spaCy library is imported for text processing. The en_core_web_sm model is downloaded from spaCy, which provides English language processing capabilities.\n",
    "The CountVectorizer from sklearn.feature_extraction.text is used to convert text data into a matrix of token counts (bag-of-words model). In essence, it converts text data into a numerical format that LatentDirichletAllocation (LDA) can understand. The LDA from sklearn.decomposition is imported for performing topic modeling. preprocess_text function is defined to tokenize the text, remove stop words as well as punctuations, and lemmatize words. The cleaned text is returned as a single string, ready for topic modelling. This is a critical step before applying Latent Dirichlet Allocation (LDA), which helps in identifying common themes and issues mentioned by customers in their feedback.\n",
    "\n",
    "The following code topic models the reviews as a whole on the ecommerce platform. Preprocessing is only done for 10000 reviews because the algorithm will over run the time for all 1000000 reviews. CountVectorizer is used to convert text to a document-term matrix. max_df=0.90 means words that appear in more than 90% of the documents will be ignored. min_df=2 means words that appear in fewer than 2 documents will also be ignored. Afterwards we resort to LDA to extract common topics. The n_components parameter determines the dimensionality of the new feature space that LDA projects the data onto. In essence, print_topics is a function defined to display the popular topics based on ALL the reviews (in route 1). There is also code provided (route 2) which displays popular topics for each (store_region, supplier) pair group. This is useful for businesses to see which store in whichever region with their respective supplier needs room for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIJMdTcbXFdM",
    "outputId": "6b64eee3-5573-4e2f-ee44-c924e70ef0c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\covan\\AppData\\Local\\Temp\\ipykernel_25008\\442618712.py:6: DtypeWarning: Columns (16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('final_data.csv', delimiter=\",\", encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  customer_key  quantity_purchased  total_price purchase_date  \\\n",
      "0      C002181                   4         56.0    2014-01-20   \n",
      "1      C005518                   4         64.0    2014-01-20   \n",
      "2      C002449                   6        102.0    2014-01-20   \n",
      "3      C005284                   4         52.0    2014-01-20   \n",
      "4      C007996                   6        102.0    2014-01-20   \n",
      "\n",
      "  time_of_purchase                                     item_name  \\\n",
      "0         14:06:00        Chinet Comfort Hot Cups with Lids 16oz   \n",
      "1         14:06:00            Chobani Greek Yogurt Variety Pack    \n",
      "2         14:06:00                  Folgers Classic Roast Coffee   \n",
      "3         14:06:00                     Spunkmeyer Muffin Variety   \n",
      "4         14:06:00  Nature Valley Biscuit Sandwich Variety Pack    \n",
      "\n",
      "           description  unit_price manufacturing_country            supplier  \\\n",
      "0  Dishware - Cups Hot        14.0         United States           MAESA SAS   \n",
      "1       Food - Healthy        16.0               Germany  Friedola 1888 GmbH   \n",
      "2        Coffee Ground        17.0               Germany  Friedola 1888 GmbH   \n",
      "3        Food - Sweets        13.0            Bangladesh        DENIMACH LTD   \n",
      "4       Food - Healthy        17.0               Finland         HARDFORD AB   \n",
      "\n",
      "   ... inventory_level inventory_cost revenue  campaign_key  mkt_chnl_key  \\\n",
      "0  ...             178            5.0    36.0           NaN           NaN   \n",
      "1  ...             253           14.0     8.0           NaN           NaN   \n",
      "2  ...             246           12.0    30.0           NaN           NaN   \n",
      "3  ...             250            3.0    40.0           NaN           NaN   \n",
      "4  ...             295           14.0    18.0           NaN           NaN   \n",
      "\n",
      "   actual_delivery_time late_delivery  \\\n",
      "0                     4             1   \n",
      "1                     3             0   \n",
      "2                     1             0   \n",
      "3                     1             0   \n",
      "4                     4             1   \n",
      "\n",
      "                                    customer_reviews  unique_row  \\\n",
      "0                 Not worth the money, poor quality.           1   \n",
      "1                Terrible service, not happy at all.           2   \n",
      "2  Delivery was slow, but customer support helped...           3   \n",
      "3               Loved it! Would recommend to others.           4   \n",
      "4              Product is okay, but could be better.           5   \n",
      "\n",
      "                           customer_review  \n",
      "0  Excellent customer service and quality.  \n",
      "1       Fast delivery, packaging was good.  \n",
      "2            Decent product for the price.  \n",
      "3            Decent product for the price.  \n",
      "4       Not worth the money, poor quality.  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (from requests->vaderSentiment) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\covan\\downloads\\dsa3101\\venv\\lib\\site-packages (from requests->vaderSentiment) (3.4.0)\n",
      "                                          customer_review sentiment\n",
      "0                 Excellent customer service and quality.  positive\n",
      "1                      Fast delivery, packaging was good.  positive\n",
      "2                           Decent product for the price.   neutral\n",
      "3                           Decent product for the price.   neutral\n",
      "4                      Not worth the money, poor quality.  negative\n",
      "...                                                   ...       ...\n",
      "998737            Excellent customer service and quality.  positive\n",
      "998738  Delivery was slow, but customer support helped...  positive\n",
      "998739            Excellent customer service and quality.  positive\n",
      "998740               Loved it! Would recommend to others.  positive\n",
      "998741                 Fast delivery, packaging was good.  positive\n",
      "\n",
      "[998742 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Natural language processing to analyze customer reviews and feedback\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('final_data.csv', delimiter=\",\", encoding='ISO-8859-1')\n",
    "\n",
    "# using sample_reviews to randomly generate data for new column 'customer_review'\n",
    "sample_reviews = [\n",
    "    \"Great product, very satisfied!\",\n",
    "    \"Terrible service, not happy at all.\",\n",
    "    \"Fast delivery, packaging was good.\",\n",
    "    \"Product is okay, but could be better.\",\n",
    "    \"Excellent customer service and quality.\",\n",
    "    \"The product arrived late, but it's good.\",\n",
    "    \"Not worth the money, poor quality.\",\n",
    "    \"Loved it! Would recommend to others.\",\n",
    "    \"Decent product for the price.\",\n",
    "    \"Delivery was slow, but customer support helped a lot.\"\n",
    "]\n",
    "\n",
    "#ensure reproducibility\n",
    "random.seed(119)\n",
    "\n",
    "df['customer_review'] = [random.choice(sample_reviews) for _ in range(len(df))]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Setting up sentiment analysis\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(review):\n",
    "    return analyzer.polarity_scores(review)\n",
    "\n",
    "df['sentiment_scores'] = df['customer_review'].apply(get_sentiment)\n",
    "\n",
    "# Extract overall sentiment (positive/negative/neutral)\n",
    "df['compound'] = df['sentiment_scores'].apply(lambda score: score['compound'])\n",
    "df['sentiment'] = df['compound'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')\n",
    "\n",
    "print(df[['customer_review', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpScH6WhaXvG"
   },
   "outputs": [],
   "source": [
    "# Topic modelling to identify common issues and suggestions from customer feedback\n",
    "\n",
    "!pip install spacy gensim\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Load spaCy's English tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# preprocessed text is returned as a single string\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Lemmatization reduces words to their base forms thus making subsequent analysis more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LaCtRTSj_xcZ",
    "outputId": "efaa835e-ead6-4d33-f01b-c02f90611764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "customer delivery service quality excellent\n",
      "Topic 1:\n",
      "product satisfied great late arrive\n"
     ]
    }
   ],
   "source": [
    "# Route 1: No groupby, topic models the reviews as a whole on the ecommerce platform\n",
    "\n",
    "# rationale for choosing only 10000 rows: too many customer_reviews (1000000) to process which causes preprocessing and LDA to overrun time\n",
    "# Preprocess each review\n",
    "preprocessed_reviews = [preprocess_text(i) for i in df['customer_review'].head(10000)]\n",
    "\n",
    "# Vectorize the preprocessed reviews using CountVectorizer to convert text data into a numerical format that LDA can understand\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english') #max_df=0.90 means words that appear in more than 90% of the documents will be ignored while min_df=2 means words that appear in fewer than 2 documents will also be ignored.\n",
    "#max_df removes words that are too frequent and is unlikely to carry meaningful information\n",
    "#min_df removes super rare words which could be typos or noise\n",
    "X = vectorizer.fit_transform(preprocessed_reviews)\n",
    "\n",
    "# Latent Dirichlet allocation (LDA) to extract popular topics\n",
    "lda = LDA(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# This function displays the popular topics\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out() # Essentially, this is a list of all the words in the vocabulary that the model uses\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "# Top 5 words that are most representative of each topic\n",
    "print_topics(lda, vectorizer, 5)\n",
    "\n",
    "# set to 2 topics and 5 words each such that one can get a sense of the main themes without overwhelming details, prevents information overload and ensures the topics are interpretable\n",
    "\n",
    "# It can be seen from topic 0 and 1 that 'delivery' and 'slow' is often associated with each other,\n",
    "# as well as 'money'+'worth' and 'quality'+'poor'. This suggest that there needs to be improvements for delivery\n",
    "# time, and ensure better quality of products. Good job on keeping the prices affordable for people to point out\n",
    "# that it is worth their money!\n",
    "\n",
    "# All in all, businesses can gain a deeper understanding of their customers’ experiences,\n",
    "# identify pain points, and respond more effectively to customer needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrHovbVcdWJW"
   },
   "outputs": [],
   "source": [
    "# Route 2: Group reviews by (store_region, supplier) pair group\n",
    "\n",
    "# Preprocess each customer_review\n",
    "# rationale for choosing only 10000 rows: too many customer_reviews (1000000) to process which causes preprocessing and LDA to overrun time\n",
    "df=df.head(10000)\n",
    "df['processed_review'] = df['customer_review'].apply(preprocess_text)\n",
    "\n",
    "grouped_reviews = df.groupby(['store_region', 'supplier'])['processed_review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Helper function to display the popular topics\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "# Define a function to apply topic modeling for each 'store_region' and 'supplier'\n",
    "def topic_modeling_for_group(grouped_reviews):\n",
    "    for _, row in grouped_reviews.iterrows():\n",
    "        store_region = row['store_region']\n",
    "        supplier = row['supplier']\n",
    "        reviews = [row['processed_review']]\n",
    "\n",
    "        # Vectorize the preprocessed reviews using CountVectorizer\n",
    "        vectorizer = CountVectorizer(max_df=1, min_df=1, stop_words='english')\n",
    "        X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "        # Latent Dirichlet allocation (LDA) to extract popular topics\n",
    "        lda = LDA(n_components=2, random_state=38)\n",
    "        lda.fit(X)\n",
    "\n",
    "        print(f\"Store Region: {store_region} | Supplier: {supplier}\")\n",
    "\n",
    "        # Top 5 words that are most representative of each topic for each 'store_region' and 'supplier' combi\n",
    "        print_topics(lda, vectorizer, 5)\n",
    "\n",
    "\n",
    "topic_modeling_for_group(grouped_reviews)\n",
    "\n",
    "\n",
    "\n",
    "# All in all, businesses can gain a deeper understanding of their customers’ experiences,\n",
    "# identify pain points, and respond more effectively to customer needs. This is useful for businesses\n",
    "# to see which store in whichever region with their respective supplier needs room for improvement.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
